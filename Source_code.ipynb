{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_linear(n: int = 100):\n",
    "    pts = np.random.uniform(0, 1, (n, 2))\n",
    "    inputs, labels = [], []\n",
    "    for pt in pts:\n",
    "        inputs.append([pt[0], pt[1]])\n",
    "        if pt[0] > pt[1]:\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "    return np.array(inputs), np.array(labels).reshape(n, 1)\n",
    "\n",
    "def generate_xor_easy(n: int = 11):\n",
    "    inputs, labels = [], []\n",
    "    for i in range(n):\n",
    "        inputs.append([0.1 * i, 0.1 * i])\n",
    "        labels.append(0)\n",
    "        if 0.1 * i == 0.5:\n",
    "            continue\n",
    "        inputs.append([0.1 * i, 1 - 0.1 * i])\n",
    "        labels.append(1)\n",
    "    return np.array(inputs), np.array(labels).reshape(21, 1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def derivative_sigmoid(x):\n",
    "    return np.multiply(x, 1.0 - x)\n",
    "\n",
    "def leaky_ReLU(x):\n",
    "    return np.where(x > 0, x, 0.01 * x)\n",
    "\n",
    "def derivative_leaky_ReLU(x):\n",
    "    return np.where(x > 0, 1, 0.01)\n",
    "\n",
    "def show_result(x, y, pred_y):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Ground truth', fontsize=18)\n",
    "    for i in range(x.shape[0]): # x.shape = (100, 2)\n",
    "        if y[i] == 0:\n",
    "            plt.plot(x[i][0], x[i][1], 'ro')\n",
    "        else:\n",
    "            plt.plot(x[i][0], x[i][1], 'bo')    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Predict result', fontsize=18)\n",
    "    for i in range(x.shape[0]):\n",
    "        if pred_y[i] == 0:\n",
    "            plt.plot(x[i][0], x[i][1], 'ro')\n",
    "        else:\n",
    "            plt.plot(x[i][0], x[i][1], 'bo')     \n",
    "    plt.show()\n",
    "\n",
    "def show_loss(loss):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 loss : 0.25042799\n",
      "epoch 200 loss : 0.24618281\n",
      "epoch 300 loss : 0.24159749\n",
      "epoch 400 loss : 0.23351941\n",
      "epoch 500 loss : 0.21586073\n",
      "epoch 600 loss : 0.18397518\n",
      "epoch 700 loss : 0.14309598\n",
      "epoch 800 loss : 0.10716997\n",
      "epoch 900 loss : 0.08249298\n",
      "epoch 1000 loss : 0.06626331\n",
      "epoch 1100 loss : 0.05510333\n",
      "epoch 1200 loss : 0.04690807\n",
      "epoch 1300 loss : 0.04047374\n",
      "epoch 1400 loss : 0.03513331\n",
      "epoch 1500 loss : 0.03052624\n",
      "epoch 1600 loss : 0.0264618\n",
      "epoch 1700 loss : 0.02284576\n",
      "epoch 1800 loss : 0.01963733\n",
      "epoch 1900 loss : 0.0168192\n",
      "epoch 2000 loss : 0.01437722\n",
      "\n",
      "Iter 0| Ground truth: 0 | prdiction: 0.03631923\n",
      "Iter 1| Ground truth: 1 | prdiction: 0.97786805\n",
      "Iter 2| Ground truth: 0 | prdiction: 0.03342104\n",
      "Iter 3| Ground truth: 1 | prdiction: 0.99090016\n",
      "Iter 4| Ground truth: 0 | prdiction: 0.04540537\n",
      "Iter 5| Ground truth: 1 | prdiction: 0.99441914\n",
      "Iter 6| Ground truth: 0 | prdiction: 0.1048517\n",
      "Iter 7| Ground truth: 1 | prdiction: 0.98784688\n",
      "Iter 8| Ground truth: 0 | prdiction: 0.2063212\n",
      "Iter 9| Ground truth: 1 | prdiction: 0.74583728\n",
      "Iter 10| Ground truth: 0 | prdiction: 0.2178219\n",
      "Iter 11| Ground truth: 0 | prdiction: 0.14883667\n",
      "Iter 12| Ground truth: 1 | prdiction: 0.68813013\n",
      "Iter 13| Ground truth: 0 | prdiction: 0.08387343\n",
      "Iter 14| Ground truth: 1 | prdiction: 0.97071177\n",
      "Iter 15| Ground truth: 0 | prdiction: 0.04626471\n",
      "Iter 16| Ground truth: 1 | prdiction: 0.99180807\n",
      "Iter 17| Ground truth: 0 | prdiction: 0.02715442\n",
      "Iter 18| Ground truth: 1 | prdiction: 0.99519559\n",
      "Iter 19| Ground truth: 0 | prdiction: 0.01736037\n",
      "Iter 20| Ground truth: 1 | prdiction: 0.99629514\n",
      "loss = 0.014354636310545274 , accuracy = 100%\n"
     ]
    }
   ],
   "source": [
    "# sigmoid activation\n",
    "\n",
    "np.random.seed(5)\n",
    "# x_train, y_train = generate_linear(n = 100) \n",
    "x_train, y_train = generate_xor_easy()\n",
    "\n",
    "layer_num = [2, 10, 10, 1]\n",
    "epoch = 2000\n",
    "learning_rate = 0.05\n",
    "\n",
    "W1 = np.random.randn(layer_num[0], layer_num[1])\n",
    "W2 = np.random.randn(layer_num[1], layer_num[2])\n",
    "W3 = np.random.randn(layer_num[2], layer_num[3])\n",
    "loss = []\n",
    "\n",
    "# train\n",
    "\n",
    "for i in range(1,epoch+1):\n",
    "\n",
    "    # forward\n",
    "    H1 = np.dot(x_train, W1)\n",
    "    Z1 = sigmoid(H1)\n",
    "    H2 = np.dot(Z1, W2)\n",
    "    Z2 = sigmoid(H2)\n",
    "    H3 = np.dot(Z2, W3)\n",
    "    y_pred = sigmoid(H3)\n",
    "\n",
    "    # calculate loss\n",
    "    loss.append(np.mean((y_pred - y_train) ** 2))\n",
    "    if i % 100 == 0:\n",
    "        print('epoch ' + str(i) + ' loss : ' + str(round(loss[i-1], 8)))\n",
    "\n",
    "    # backward\n",
    "    cache_y = derivative_sigmoid(y_pred) * 2 * (y_pred - y_train)\n",
    "    dW3 = np.dot(Z2.T, cache_y)\n",
    "    temp1 = np.dot(cache_y, W3.T)\n",
    "    cache_layer2 = derivative_sigmoid(Z2) * temp1\n",
    "    dW2 = np.dot(Z1.T, cache_layer2)\n",
    "    temp2 = np.dot(cache_layer2, W2.T)\n",
    "    cache_layer1 = derivative_sigmoid(Z1) * temp2\n",
    "    dW1 = np.dot(x_train.T, cache_layer1)\n",
    "\n",
    "    # update weight\n",
    "    W1 -= learning_rate * dW1\n",
    "    W2 -= learning_rate * dW2\n",
    "    W3 -= learning_rate * dW3\n",
    "\n",
    "# test\n",
    "\n",
    "H1 = np.dot(x_train, W1)\n",
    "Z1 = sigmoid(H1)\n",
    "H2 = np.dot(Z1, W2)\n",
    "Z2 = sigmoid(H2)\n",
    "H3 = np.dot(Z2, W3)\n",
    "y_pred = sigmoid(H3)\n",
    "print()\n",
    "for i in range(y_train.shape[0]):\n",
    "    print('Iter '+str(i)+'| Ground truth: ' + str(y_train[i][0]) + ' | prdiction: ' + str(round(y_pred[i][0], 8)))\n",
    "\n",
    "test_loss = np.mean((y_pred - y_train) ** 2)\n",
    "y_pred = np.round(y_pred)\n",
    "acc = np.sum((y_train == y_pred) / y_train.shape[0]) * 100\n",
    "print('loss = ' + str(test_loss) + ' , accuracy = ' + str(round(acc)) + '%')\n",
    "\n",
    "# show_result(x_train, y_train, y_pred)\n",
    "# show_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 loss : 0.12887728392156628\n",
      "epoch 200 loss : 0.12887728392156414\n",
      "epoch 300 loss : 0.1288772839215642\n",
      "epoch 400 loss : 0.1288772839215642\n",
      "epoch 500 loss : 0.1288772839215642\n",
      "epoch 600 loss : 0.1288772839215642\n",
      "epoch 700 loss : 0.1288772839215642\n",
      "epoch 800 loss : 0.1288772839215642\n",
      "epoch 900 loss : 0.1288772839215642\n",
      "epoch 1000 loss : 0.1288772839215642\n",
      "epoch 1100 loss : 0.1288772839215642\n",
      "epoch 1200 loss : 0.1288772839215642\n",
      "epoch 1300 loss : 0.1288772839215642\n",
      "epoch 1400 loss : 0.1288772839215642\n",
      "epoch 1500 loss : 0.1288772839215642\n",
      "epoch 1600 loss : 0.1288772839215642\n",
      "epoch 1700 loss : 0.1288772839215642\n",
      "epoch 1800 loss : 0.1288772839215642\n",
      "epoch 1900 loss : 0.1288772839215642\n",
      "epoch 2000 loss : 0.1288772839215642\n",
      "\n",
      "Iter 0| Ground truth: 1 | prediction: 1.1605058740170306\n",
      "Iter 1| Ground truth: 1 | prediction: 0.8640129379820698\n",
      "Iter 2| Ground truth: 0 | prediction: 0.26643786160828226\n",
      "Iter 3| Ground truth: 0 | prediction: -0.1783674808585788\n",
      "Iter 4| Ground truth: 1 | prediction: 0.6169260267803571\n",
      "Iter 5| Ground truth: 1 | prediction: 0.8488018084552459\n",
      "Iter 6| Ground truth: 0 | prediction: -0.11834703180609565\n",
      "Iter 7| Ground truth: 1 | prediction: 1.2383489448809069\n",
      "Iter 8| Ground truth: 1 | prediction: 0.5749828233946195\n",
      "Iter 9| Ground truth: 0 | prediction: -0.49978943571796847\n",
      "Iter 10| Ground truth: 1 | prediction: 1.1203857145064147\n",
      "Iter 11| Ground truth: 1 | prediction: 0.714045030273519\n",
      "Iter 12| Ground truth: 0 | prediction: -0.3198010337137082\n",
      "Iter 13| Ground truth: 1 | prediction: 0.8564971171925224\n",
      "Iter 14| Ground truth: 0 | prediction: 0.335764354558583\n",
      "Iter 15| Ground truth: 1 | prediction: 0.6396007361229142\n",
      "Iter 16| Ground truth: 1 | prediction: 0.5221210222806899\n",
      "Iter 17| Ground truth: 1 | prediction: 1.0827061496423445\n",
      "Iter 18| Ground truth: 0 | prediction: 0.04068957045484133\n",
      "Iter 19| Ground truth: 0 | prediction: 0.09538411504820422\n",
      "Iter 20| Ground truth: 0 | prediction: 0.28343976816706573\n",
      "Iter 21| Ground truth: 0 | prediction: 0.19203666351432677\n",
      "Iter 22| Ground truth: 1 | prediction: 0.8456614974968724\n",
      "Iter 23| Ground truth: 1 | prediction: 1.1612898881639586\n",
      "Iter 24| Ground truth: 1 | prediction: 1.0403820136054551\n",
      "Iter 25| Ground truth: 1 | prediction: 0.4145362417719505\n",
      "Iter 26| Ground truth: 0 | prediction: 0.44096915829800676\n",
      "Iter 27| Ground truth: 0 | prediction: -0.23760473436875232\n",
      "Iter 28| Ground truth: 1 | prediction: 1.0423567617100622\n",
      "Iter 29| Ground truth: 1 | prediction: 0.772167662416238\n",
      "Iter 30| Ground truth: 0 | prediction: 0.5025141737509399\n",
      "Iter 31| Ground truth: 1 | prediction: 0.4463252261203207\n",
      "Iter 32| Ground truth: 0 | prediction: 0.020625838776476845\n",
      "Iter 33| Ground truth: 0 | prediction: 0.9018975754317516\n",
      "Iter 34| Ground truth: 0 | prediction: 0.23913663602098909\n",
      "Iter 35| Ground truth: 0 | prediction: -0.2838775358419954\n",
      "Iter 36| Ground truth: 1 | prediction: 0.5366633344356448\n",
      "Iter 37| Ground truth: 0 | prediction: 0.32157896455783397\n",
      "Iter 38| Ground truth: 1 | prediction: 0.45148876052767406\n",
      "Iter 39| Ground truth: 0 | prediction: 0.27648605536064746\n",
      "Iter 40| Ground truth: 1 | prediction: 0.5451692456782526\n",
      "Iter 41| Ground truth: 1 | prediction: 0.9378431591605745\n",
      "Iter 42| Ground truth: 0 | prediction: 0.34652804356458566\n",
      "Iter 43| Ground truth: 1 | prediction: 0.9357298495462888\n",
      "Iter 44| Ground truth: 0 | prediction: 0.5295738926998208\n",
      "Iter 45| Ground truth: 1 | prediction: 0.9878059986388529\n",
      "Iter 46| Ground truth: 1 | prediction: 1.3290072761242486\n",
      "Iter 47| Ground truth: 1 | prediction: 0.5749790842191022\n",
      "Iter 48| Ground truth: 0 | prediction: 0.647270181839514\n",
      "Iter 49| Ground truth: 1 | prediction: 0.9324077105351012\n",
      "Iter 50| Ground truth: 1 | prediction: 0.5114801022528306\n",
      "Iter 51| Ground truth: 0 | prediction: 0.19078133379859297\n",
      "Iter 52| Ground truth: 1 | prediction: 0.7779491599595371\n",
      "Iter 53| Ground truth: 0 | prediction: 0.6999647378668054\n",
      "Iter 54| Ground truth: 1 | prediction: 0.6449730488888016\n",
      "Iter 55| Ground truth: 0 | prediction: -0.18197676812132949\n",
      "Iter 56| Ground truth: 1 | prediction: 0.6257645594164514\n",
      "Iter 57| Ground truth: 1 | prediction: 0.9758036651641345\n",
      "Iter 58| Ground truth: 1 | prediction: 0.28183769190627017\n",
      "Iter 59| Ground truth: 0 | prediction: -0.17638982915157955\n",
      "Iter 60| Ground truth: 1 | prediction: 0.7583762537110473\n",
      "Iter 61| Ground truth: 1 | prediction: 1.1101963183155177\n",
      "Iter 62| Ground truth: 0 | prediction: -0.44333019119611117\n",
      "Iter 63| Ground truth: 1 | prediction: 0.923045825421045\n",
      "Iter 64| Ground truth: 0 | prediction: -0.14003897461959516\n",
      "Iter 65| Ground truth: 1 | prediction: 1.2563541687910738\n",
      "Iter 66| Ground truth: 0 | prediction: -0.22042062175393912\n",
      "Iter 67| Ground truth: 1 | prediction: 0.8556711705464817\n",
      "Iter 68| Ground truth: 0 | prediction: -0.2287113669018178\n",
      "Iter 69| Ground truth: 1 | prediction: 0.8633719149634587\n",
      "Iter 70| Ground truth: 0 | prediction: -0.06332987384786759\n",
      "Iter 71| Ground truth: 1 | prediction: 0.4143031692731057\n",
      "Iter 72| Ground truth: 0 | prediction: -0.3978150112520328\n",
      "Iter 73| Ground truth: 1 | prediction: 0.5537808139190894\n",
      "Iter 74| Ground truth: 1 | prediction: 0.19411265112189757\n",
      "Iter 75| Ground truth: 1 | prediction: 0.7970445563417896\n",
      "Iter 76| Ground truth: 0 | prediction: 0.2569018257995018\n",
      "Iter 77| Ground truth: 1 | prediction: 0.43401005710475427\n",
      "Iter 78| Ground truth: 1 | prediction: 1.111659360780425\n",
      "Iter 79| Ground truth: 0 | prediction: -0.20054466801849502\n",
      "Iter 80| Ground truth: 1 | prediction: 1.2768388438325888\n",
      "Iter 81| Ground truth: 0 | prediction: 0.17273738137889125\n",
      "Iter 82| Ground truth: 1 | prediction: 0.8919738040561953\n",
      "Iter 83| Ground truth: 1 | prediction: 0.9954403936439893\n",
      "Iter 84| Ground truth: 1 | prediction: 0.40567247080918506\n",
      "Iter 85| Ground truth: 0 | prediction: 0.026085235074549552\n",
      "Iter 86| Ground truth: 0 | prediction: -0.47938771868911967\n",
      "Iter 87| Ground truth: 0 | prediction: -0.27856084494079575\n",
      "Iter 88| Ground truth: 1 | prediction: 0.4372348801034418\n",
      "Iter 89| Ground truth: 1 | prediction: 0.6998715511292071\n",
      "Iter 90| Ground truth: 0 | prediction: 0.18045473745568497\n",
      "Iter 91| Ground truth: 0 | prediction: -0.17860646643531647\n",
      "Iter 92| Ground truth: 1 | prediction: 0.4362832979388987\n",
      "Iter 93| Ground truth: 0 | prediction: 0.09991047971806435\n",
      "Iter 94| Ground truth: 0 | prediction: 0.7479966849794314\n",
      "Iter 95| Ground truth: 1 | prediction: 0.5089084938115843\n",
      "Iter 96| Ground truth: 1 | prediction: 0.5622208602234765\n",
      "Iter 97| Ground truth: 1 | prediction: 0.6153446287380441\n",
      "Iter 98| Ground truth: 0 | prediction: 0.05752769707270981\n",
      "Iter 99| Ground truth: 0 | prediction: 0.5441771992492588\n",
      "loss = 0.1288772839215642 , accuracy = 83%\n"
     ]
    }
   ],
   "source": [
    "# without activation\n",
    "\n",
    "np.random.seed(5)\n",
    "x_train, y_train = generate_linear(n = 100) \n",
    "# x_train, y_train = generate_xor_easy()\n",
    "\n",
    "layer_num = [2, 10, 10, 1]\n",
    "epoch = 2000\n",
    "learning_rate = 0.00025\n",
    "\n",
    "W1 = np.random.randn(layer_num[0], layer_num[1])\n",
    "W2 = np.random.randn(layer_num[1], layer_num[2])\n",
    "W3 = np.random.randn(layer_num[2], layer_num[3])\n",
    "loss = []\n",
    "\n",
    "# train\n",
    "\n",
    "for i in range(1,epoch+1):\n",
    "\n",
    "    # forward\n",
    "    H1 = np.dot(x_train, W1)\n",
    "    H2 = np.dot(H1, W2)\n",
    "    y_pred = np.dot(H2, W3)\n",
    "\n",
    "    # calculate loss\n",
    "    loss.append(np.mean((y_pred - y_train) ** 2))\n",
    "    if i % 100 == 0 :\n",
    "        print('epoch ' + str(i) + ' loss : ' + str(loss[i-1]))\n",
    "\n",
    "    # backward\n",
    "    dW3 = np.dot(H2.T, (y_pred - y_train))\n",
    "    cache_layer2 = np.dot((y_pred - y_train), W3.T)\n",
    "    dW2 = np.dot(H1.T, cache_layer2)\n",
    "    cache_layer1 = np.dot(cache_layer2, W2.T)\n",
    "    dW1 = np.dot(x_train.T, cache_layer1)\n",
    "\n",
    "    # update weight\n",
    "    W1 -= learning_rate * dW1\n",
    "    W2 -= learning_rate * dW2\n",
    "    W3 -= learning_rate * dW3\n",
    "\n",
    "# test\n",
    "\n",
    "H1 = np.dot(x_train, W1)\n",
    "H2 = np.dot(H1, W2)\n",
    "y_pred = np.dot(H2, W3)\n",
    "print()\n",
    "for i in range(y_train.shape[0]) :\n",
    "    print('Iter '+str(i)+'| Ground truth: ' + str(y_train[i][0]) + ' | prediction: ' + str(y_pred[i][0]))\n",
    "\n",
    "test_loss = np.mean((y_pred - y_train) ** 2)\n",
    "y_pred = np.round(y_pred)\n",
    "acc = np.sum((y_train == y_pred) / y_train.shape[0]) * 100\n",
    "print('loss = ' + str(test_loss) + ' , accuracy = ' + str(round(acc)) + '%')\n",
    "\n",
    "# show_result(x_train, y_train, y_pred)\n",
    "# show_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 loss : 0.00144906\n",
      "epoch 200 loss : 0.00062289\n",
      "epoch 300 loss : 0.00037615\n",
      "epoch 400 loss : 0.00025906\n",
      "epoch 500 loss : 0.00019284\n",
      "epoch 600 loss : 0.00015116\n",
      "epoch 700 loss : 0.00012293\n",
      "epoch 800 loss : 0.00010275\n",
      "epoch 900 loss : 8.772e-05\n",
      "epoch 1000 loss : 7.616e-05\n",
      "epoch 1100 loss : 6.704e-05\n",
      "epoch 1200 loss : 5.968e-05\n",
      "epoch 1300 loss : 5.364e-05\n",
      "epoch 1400 loss : 4.861e-05\n",
      "epoch 1500 loss : 4.436e-05\n",
      "epoch 1600 loss : 4.072e-05\n",
      "epoch 1700 loss : 3.759e-05\n",
      "epoch 1800 loss : 3.486e-05\n",
      "epoch 1900 loss : 3.246e-05\n",
      "epoch 2000 loss : 3.035e-05\n",
      "\n",
      "Iter 0| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 1| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 2| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 3| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 4| Ground truth: 0 | prdiction: 1e-08\n",
      "Iter 5| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 6| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 7| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 8| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 9| Ground truth: 0 | prdiction: 4.56e-06\n",
      "Iter 10| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 11| Ground truth: 0 | prdiction: 0.00073957\n",
      "Iter 12| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 13| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 14| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 15| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 16| Ground truth: 1 | prdiction: 0.99999985\n",
      "Iter 17| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 18| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 19| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 20| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 21| Ground truth: 1 | prdiction: 0.99995227\n",
      "Iter 22| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 23| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 24| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 25| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 26| Ground truth: 1 | prdiction: 0.99999953\n",
      "Iter 27| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 28| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 29| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 30| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 31| Ground truth: 0 | prdiction: 0.00127838\n",
      "Iter 32| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 33| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 34| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 35| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 36| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 37| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 38| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 39| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 40| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 41| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 42| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 43| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 44| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 45| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 46| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 47| Ground truth: 0 | prdiction: 3.3e-07\n",
      "Iter 48| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 49| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 50| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 51| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 52| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 53| Ground truth: 1 | prdiction: 0.99944093\n",
      "Iter 54| Ground truth: 1 | prdiction: 0.99291743\n",
      "Iter 55| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 56| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 57| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 58| Ground truth: 1 | prdiction: 0.99999881\n",
      "Iter 59| Ground truth: 0 | prdiction: 0.00720491\n",
      "Iter 60| Ground truth: 0 | prdiction: 0.01826744\n",
      "Iter 61| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 62| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 63| Ground truth: 0 | prdiction: 0.02074476\n",
      "Iter 64| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 65| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 66| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 67| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 68| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 69| Ground truth: 1 | prdiction: 0.99949006\n",
      "Iter 70| Ground truth: 0 | prdiction: 0.02631704\n",
      "Iter 71| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 72| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 73| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 74| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 75| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 76| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 77| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 78| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 79| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 80| Ground truth: 0 | prdiction: 4.288e-05\n",
      "Iter 81| Ground truth: 0 | prdiction: 1e-08\n",
      "Iter 82| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 83| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 84| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 85| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 86| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 87| Ground truth: 0 | prdiction: 1e-08\n",
      "Iter 88| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 89| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 90| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 91| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 92| Ground truth: 0 | prdiction: 6e-08\n",
      "Iter 93| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 94| Ground truth: 1 | prdiction: 0.99999998\n",
      "Iter 95| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 96| Ground truth: 1 | prdiction: 0.96164316\n",
      "Iter 97| Ground truth: 1 | prdiction: 1.0\n",
      "Iter 98| Ground truth: 0 | prdiction: 0.0\n",
      "Iter 99| Ground truth: 0 | prdiction: 0.0\n",
      "loss = 3.0327104001723564e-05 , accuracy = 100%\n"
     ]
    }
   ],
   "source": [
    "# momentum optimizer\n",
    "\n",
    "np.random.seed(5)\n",
    "x_train, y_train = generate_linear(n = 100) \n",
    "# x_train, y_train = generate_xor_easy()\n",
    "\n",
    "layer_num = [2, 10, 10, 1]\n",
    "epoch = 2000\n",
    "learning_rate = 0.05\n",
    "\n",
    "W1 = np.random.randn(layer_num[0], layer_num[1])\n",
    "W2 = np.random.randn(layer_num[1], layer_num[2])\n",
    "W3 = np.random.randn(layer_num[2], layer_num[3])\n",
    "v1 = 0\n",
    "v2 = 0\n",
    "v3 = 0\n",
    "beta = 0.9\n",
    "loss = []\n",
    "\n",
    "# train\n",
    "\n",
    "for i in range(1,epoch+1):\n",
    "\n",
    "    # forward\n",
    "    H1 = np.dot(x_train, W1)\n",
    "    Z1 = sigmoid(H1)\n",
    "    H2 = np.dot(Z1, W2)\n",
    "    Z2 = sigmoid(H2)\n",
    "    H3 = np.dot(Z2, W3)\n",
    "    y_pred = sigmoid(H3)\n",
    "\n",
    "    # calculate loss\n",
    "    loss.append(np.mean((y_pred - y_train) ** 2))\n",
    "    if i % 100 == 0:\n",
    "        print('epoch ' + str(i) + ' loss : ' + str(round(loss[i-1], 8)))\n",
    "\n",
    "    # backward\n",
    "    cache_y = derivative_sigmoid(y_pred) * 2 * (y_pred - y_train)\n",
    "    dW3 = np.dot(Z2.T, cache_y)\n",
    "    temp1 = np.dot(cache_y, W3.T)\n",
    "    cache_layer2 = derivative_sigmoid(Z2) * temp1\n",
    "    dW2 = np.dot(Z1.T, cache_layer2)\n",
    "    temp2 = np.dot(cache_layer2, W2.T)\n",
    "    cache_layer1 = derivative_sigmoid(Z1) * temp2\n",
    "    dW1 = np.dot(x_train.T, cache_layer1)\n",
    "\n",
    "    # update weight\n",
    "    v1 = beta * v1 - learning_rate * dW1\n",
    "    v2 = beta * v2 - learning_rate * dW2\n",
    "    v3 = beta * v3 - learning_rate * dW3\n",
    "    W1 += v1\n",
    "    W2 += v2\n",
    "    W3 += v3\n",
    "\n",
    "# test\n",
    "\n",
    "H1 = np.dot(x_train, W1)\n",
    "Z1 = sigmoid(H1)\n",
    "H2 = np.dot(Z1, W2)\n",
    "Z2 = sigmoid(H2)\n",
    "H3 = np.dot(Z2, W3)\n",
    "y_pred = sigmoid(H3)\n",
    "print()\n",
    "for i in range(y_train.shape[0]):\n",
    "    print('Iter '+str(i)+'| Ground truth: ' + str(y_train[i][0]) + ' | prdiction: ' + str(round(y_pred[i][0], 8)))\n",
    "\n",
    "test_loss = np.mean((y_pred - y_train) ** 2)\n",
    "y_pred = np.round(y_pred)\n",
    "acc = np.sum((y_train == y_pred) / y_train.shape[0]) * 100\n",
    "print('loss = ' + str(test_loss) + ' , accuracy = ' + str(round(acc)) + '%')\n",
    "\n",
    "# show_result(x_train, y_train, y_pred)\n",
    "# show_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 loss : 0.09104941890115141\n",
      "epoch 200 loss : 0.08721681743385858\n",
      "epoch 300 loss : 0.08534520280407179\n",
      "epoch 400 loss : 0.08607306280302195\n",
      "epoch 500 loss : 0.09141405262615404\n",
      "epoch 600 loss : 0.08721693692219007\n",
      "epoch 700 loss : 0.08517860759370142\n",
      "epoch 800 loss : 0.08391944342643778\n",
      "epoch 900 loss : 0.08374030612935826\n",
      "epoch 1000 loss : 0.08278563425321935\n",
      "epoch 1100 loss : 0.08321132497914555\n",
      "epoch 1200 loss : 0.08362352998249384\n",
      "epoch 1300 loss : 0.08403783172337834\n",
      "epoch 1400 loss : 0.08838347073859207\n",
      "epoch 1500 loss : 0.08776495371868022\n",
      "epoch 1600 loss : 0.09670664651718726\n",
      "epoch 1700 loss : 0.09485184099483865\n",
      "epoch 1800 loss : 0.09055051163136861\n",
      "epoch 1900 loss : 0.09154469186811287\n",
      "epoch 2000 loss : 0.08778532557749141\n",
      "[[ 2.78097077e-05 -6.53816744e-06 -2.47127106e-03 -8.56067717e-05\n",
      "   3.53443594e-03  2.66629394e-05 -1.25253352e-04  3.52447694e-03\n",
      "  -2.75397240e-05  1.41701471e-05]\n",
      " [-9.65997450e-03 -9.53263996e-05 -3.66046004e-02 -3.90965340e-04\n",
      "   3.94494930e-04  1.22860123e-04 -5.54007994e-04  3.93383359e-04\n",
      "  -1.92828633e-04  6.84870680e-05]\n",
      " [ 2.59529910e-04  3.81785942e-06  1.57628874e-03 -1.29370704e-04\n",
      "  -4.11908659e-04  4.05626260e-05 -1.83562449e-04 -4.10748021e-04\n",
      "  -1.44793320e-05  2.18229022e-05]\n",
      " [-6.49323679e-03 -9.79627882e-05 -3.76161533e-02 -4.03019151e-04\n",
      "   1.24069806e-02  1.26641824e-04 -5.71195005e-04  1.23720214e-02\n",
      "  -1.98742493e-04  7.05783903e-05]\n",
      " [ 2.06806590e-04  6.02491879e-06  2.31199000e-03  2.74131981e-05\n",
      "  -1.56736273e-03 -8.59318188e-06  3.92364531e-05 -1.56294636e-03\n",
      "   1.31653585e-05 -4.73909732e-06]\n",
      " [-3.38059253e-03 -4.15289309e-05 -1.59465635e-02 -1.70674691e-04\n",
      "   2.63952445e-03  5.36324109e-05 -2.41881374e-04  2.63208705e-03\n",
      "  -8.41024072e-05  2.98920072e-05]\n",
      " [ 9.23323845e-05  1.98701362e-06  7.63368699e-04  7.89889497e-06\n",
      "  -4.15874679e-04 -2.47835177e-06  1.12676723e-05 -4.14702866e-04\n",
      "   4.16477396e-06 -1.37344621e-06]\n",
      " [-5.81082169e-03 -8.62433447e-05 -3.31160905e-02 -3.54778966e-04\n",
      "   1.05288827e-02  1.11483302e-04 -5.02822547e-04  1.04992154e-02\n",
      "  -1.74906793e-04  6.21307811e-05]\n",
      " [ 2.73617231e-04  4.09323720e-06  1.68360323e-03 -1.29037747e-04\n",
      "  -4.63960865e-04  4.04938059e-05 -1.82905158e-04 -4.62653559e-04\n",
      "  -1.34234067e-05  2.18474539e-05]\n",
      " [-8.27787474e-04 -4.57431693e-05 -1.75391805e-02 -2.29917250e-04\n",
      "   1.50353692e-02  7.19865540e-05 -3.30510426e-04  1.49930040e-02\n",
      "  -1.05685496e-04  3.94580149e-05]]\n",
      "\n",
      "Iter 0| Ground truth: 1 | prdiction: 1.32645459\n",
      "Iter 1| Ground truth: 1 | prdiction: 1.41653052\n",
      "Iter 2| Ground truth: 1 | prdiction: 0.83083049\n",
      "Iter 3| Ground truth: 0 | prdiction: -0.00928348\n",
      "Iter 4| Ground truth: 0 | prdiction: -0.00441587\n",
      "Iter 5| Ground truth: 1 | prdiction: 1.19198277\n",
      "Iter 6| Ground truth: 0 | prdiction: -0.0190219\n",
      "Iter 7| Ground truth: 0 | prdiction: -0.04140968\n",
      "Iter 8| Ground truth: 0 | prdiction: -0.00438871\n",
      "Iter 9| Ground truth: 0 | prdiction: 0.1381719\n",
      "Iter 10| Ground truth: 0 | prdiction: -0.02078471\n",
      "Iter 11| Ground truth: 0 | prdiction: 0.00296788\n",
      "Iter 12| Ground truth: 0 | prdiction: -0.01144621\n",
      "Iter 13| Ground truth: 1 | prdiction: 1.51841194\n",
      "Iter 14| Ground truth: 0 | prdiction: -0.0521889\n",
      "Iter 15| Ground truth: 1 | prdiction: 0.32898131\n",
      "Iter 16| Ground truth: 1 | prdiction: 1.06850018\n",
      "Iter 17| Ground truth: 1 | prdiction: 0.98343176\n",
      "Iter 18| Ground truth: 1 | prdiction: 0.90266353\n",
      "Iter 19| Ground truth: 1 | prdiction: 1.2849721\n",
      "Iter 20| Ground truth: 1 | prdiction: 1.18851587\n",
      "Iter 21| Ground truth: 1 | prdiction: 1.12193995\n",
      "Iter 22| Ground truth: 1 | prdiction: 0.82118035\n",
      "Iter 23| Ground truth: 0 | prdiction: -0.03760565\n",
      "Iter 24| Ground truth: 0 | prdiction: -0.00992478\n",
      "Iter 25| Ground truth: 0 | prdiction: -0.01204934\n",
      "Iter 26| Ground truth: 1 | prdiction: 0.12741336\n",
      "Iter 27| Ground truth: 1 | prdiction: 0.35318032\n",
      "Iter 28| Ground truth: 0 | prdiction: -0.01199936\n",
      "Iter 29| Ground truth: 0 | prdiction: -0.00953168\n",
      "Iter 30| Ground truth: 1 | prdiction: 1.0979841\n",
      "Iter 31| Ground truth: 0 | prdiction: 0.65080521\n",
      "Iter 32| Ground truth: 0 | prdiction: -0.04886\n",
      "Iter 33| Ground truth: 0 | prdiction: -0.02172901\n",
      "Iter 34| Ground truth: 1 | prdiction: 1.04871986\n",
      "Iter 35| Ground truth: 1 | prdiction: 0.28551514\n",
      "Iter 36| Ground truth: 0 | prdiction: -0.00447385\n",
      "Iter 37| Ground truth: 0 | prdiction: -0.02756565\n",
      "Iter 38| Ground truth: 0 | prdiction: -0.0045336\n",
      "Iter 39| Ground truth: 1 | prdiction: 1.2163517\n",
      "Iter 40| Ground truth: 0 | prdiction: -0.05565909\n",
      "Iter 41| Ground truth: 0 | prdiction: -0.03200633\n",
      "Iter 42| Ground truth: 1 | prdiction: 0.5821579\n",
      "Iter 43| Ground truth: 1 | prdiction: 1.29865179\n",
      "Iter 44| Ground truth: 0 | prdiction: -0.02409872\n",
      "Iter 45| Ground truth: 0 | prdiction: -0.05015706\n",
      "Iter 46| Ground truth: 1 | prdiction: 1.13166753\n",
      "Iter 47| Ground truth: 0 | prdiction: -0.00066989\n",
      "Iter 48| Ground truth: 0 | prdiction: -0.0524125\n",
      "Iter 49| Ground truth: 0 | prdiction: -0.00593928\n",
      "Iter 50| Ground truth: 0 | prdiction: -0.0045248\n",
      "Iter 51| Ground truth: 1 | prdiction: 0.28680202\n",
      "Iter 52| Ground truth: 1 | prdiction: 0.91502077\n",
      "Iter 53| Ground truth: 1 | prdiction: 0.485911\n",
      "Iter 54| Ground truth: 1 | prdiction: 1.00696754\n",
      "Iter 55| Ground truth: 1 | prdiction: 1.25748029\n",
      "Iter 56| Ground truth: 0 | prdiction: -0.01681624\n",
      "Iter 57| Ground truth: 1 | prdiction: 0.89718928\n",
      "Iter 58| Ground truth: 1 | prdiction: 1.2956578\n",
      "Iter 59| Ground truth: 0 | prdiction: 0.54730843\n",
      "Iter 60| Ground truth: 0 | prdiction: 0.51639752\n",
      "Iter 61| Ground truth: 1 | prdiction: 1.31943993\n",
      "Iter 62| Ground truth: 0 | prdiction: -0.01969867\n",
      "Iter 63| Ground truth: 0 | prdiction: 0.05604715\n",
      "Iter 64| Ground truth: 1 | prdiction: 1.13649332\n",
      "Iter 65| Ground truth: 1 | prdiction: 0.71916036\n",
      "Iter 66| Ground truth: 0 | prdiction: -0.03757442\n",
      "Iter 67| Ground truth: 1 | prdiction: 0.36777651\n",
      "Iter 68| Ground truth: 1 | prdiction: 1.30593432\n",
      "Iter 69| Ground truth: 1 | prdiction: 0.26692463\n",
      "Iter 70| Ground truth: 0 | prdiction: 0.63451156\n",
      "Iter 71| Ground truth: 1 | prdiction: 1.19094585\n",
      "Iter 72| Ground truth: 1 | prdiction: 1.61414727\n",
      "Iter 73| Ground truth: 0 | prdiction: -0.01499587\n",
      "Iter 74| Ground truth: 0 | prdiction: -0.00453952\n",
      "Iter 75| Ground truth: 0 | prdiction: -0.05874691\n",
      "Iter 76| Ground truth: 0 | prdiction: -0.00265268\n",
      "Iter 77| Ground truth: 0 | prdiction: -0.02732581\n",
      "Iter 78| Ground truth: 1 | prdiction: 0.91381442\n",
      "Iter 79| Ground truth: 1 | prdiction: 1.27378813\n",
      "Iter 80| Ground truth: 0 | prdiction: 0.06738085\n",
      "Iter 81| Ground truth: 0 | prdiction: -0.00215837\n",
      "Iter 82| Ground truth: 0 | prdiction: -0.00398501\n",
      "Iter 83| Ground truth: 0 | prdiction: -0.01810422\n",
      "Iter 84| Ground truth: 1 | prdiction: 1.01897364\n",
      "Iter 85| Ground truth: 1 | prdiction: 1.33569774\n",
      "Iter 86| Ground truth: 1 | prdiction: 0.57981503\n",
      "Iter 87| Ground truth: 0 | prdiction: -0.00353653\n",
      "Iter 88| Ground truth: 1 | prdiction: 0.64318351\n",
      "Iter 89| Ground truth: 0 | prdiction: -0.01627036\n",
      "Iter 90| Ground truth: 1 | prdiction: 0.42408967\n",
      "Iter 91| Ground truth: 1 | prdiction: 0.44937513\n",
      "Iter 92| Ground truth: 0 | prdiction: -0.00015113\n",
      "Iter 93| Ground truth: 0 | prdiction: -0.02318697\n",
      "Iter 94| Ground truth: 1 | prdiction: 0.88077499\n",
      "Iter 95| Ground truth: 0 | prdiction: -0.00184425\n",
      "Iter 96| Ground truth: 1 | prdiction: 0.61163538\n",
      "Iter 97| Ground truth: 1 | prdiction: 1.08658592\n",
      "Iter 98| Ground truth: 0 | prdiction: -0.01690357\n",
      "Iter 99| Ground truth: 0 | prdiction: -0.0118914\n",
      "loss = 0.08616193952311933 , accuracy = 84%\n"
     ]
    }
   ],
   "source": [
    "# leaky_ReLU activation\n",
    "\n",
    "np.random.seed(5)\n",
    "x_train, y_train = generate_linear(n = 100) \n",
    "# x_train, y_train = generate_xor_easy()\n",
    "\n",
    "layer_num = [2, 10, 10, 1]\n",
    "epoch = 2000\n",
    "learning_rate = 0.01\n",
    "\n",
    "W1 = np.random.randn(layer_num[0], layer_num[1])\n",
    "W2 = np.random.randn(layer_num[1], layer_num[2])\n",
    "W3 = np.random.randn(layer_num[2], layer_num[3])\n",
    "loss = []\n",
    "\n",
    "# train\n",
    "\n",
    "for i in range(1,epoch+1):\n",
    "\n",
    "    # forward\n",
    "    H1 = np.dot(x_train, W1)\n",
    "    Z1 = leaky_ReLU(H1)\n",
    "    H2 = np.dot(Z1, W2)\n",
    "    Z2 = leaky_ReLU(H2)\n",
    "    H3 = np.dot(Z2, W3)\n",
    "    y_pred = leaky_ReLU(H3)\n",
    "\n",
    "    # calculate loss\n",
    "    loss.append(np.mean((y_pred - y_train) ** 2))\n",
    "    if i % 100 == 0:\n",
    "        print('epoch ' + str(i) + ' loss : ' + str(loss[i-1]))\n",
    "\n",
    "    # backward\n",
    "    temp1 = derivative_leaky_ReLU(y_pred) * (y_pred - y_train)\n",
    "    dW3 = np.dot(Z2.T, temp1)\n",
    "    cache_layer2 = np.dot(temp1, W3.T)\n",
    "    temp2 = derivative_leaky_ReLU(Z2) * cache_layer2\n",
    "    dW2 = np.dot(Z1.T, temp2)\n",
    "    cache_layer1 = np.dot(temp2, W2.T)\n",
    "    dW1 = np.dot(x_train.T, cache_layer1)\n",
    "\n",
    "    # update weight\n",
    "    W1 -= learning_rate * dW1\n",
    "    W2 -= learning_rate * dW2\n",
    "    W3 -= learning_rate * dW3\n",
    "# test\n",
    "\n",
    "H1 = np.dot(x_train, W1)\n",
    "Z1 = leaky_ReLU(H1)\n",
    "H2 = np.dot(Z1, W2)\n",
    "Z2 = leaky_ReLU(H2)\n",
    "H3 = np.dot(Z2, W3)\n",
    "y_pred = leaky_ReLU(H3)\n",
    "print()\n",
    "for i in range(y_train.shape[0]):\n",
    "    print('Iter '+str(i)+'| Ground truth: ' + str(y_train[i][0]) + ' | prdiction: ' + str(round(y_pred[i][0], 8)))\n",
    "\n",
    "test_loss = np.mean((y_pred - y_train) ** 2)\n",
    "y_pred = np.round(y_pred)\n",
    "acc = np.sum((y_train == y_pred) / y_train.shape[0]) * 100\n",
    "print('loss = ' + str(test_loss) + ' , accuracy = ' + str(round(acc)) + '%')\n",
    "\n",
    "# show_result(x_train, y_train, y_pred)\n",
    "# show_loss(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
